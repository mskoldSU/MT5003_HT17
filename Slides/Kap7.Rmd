---
title: "Statistisk inferensteori <br> Modellval <br> (H & B, Kapitel 7)"
author: "Martin Sköld"
output:
  ioslides_presentation:
    logo: SU_logo_CMYK.png
    incremental: no
    css: slides.css
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
handout <- FALSE
```
```{r, echo = handout}
# Data och förberedelse

set.seed(1) # För att kunna reproducera

# Luftkonditionering, tider mellan fel (från Cox, D.R. and Snell, E.J. (1981))
ac <- c(90, 10, 60, 186, 61, 49, 14, 24, 56, 20, 79, 84, 44, 59, 29, 118, 25, 156, 310, 76, 26, 44, 23, 62, 130, 208, 70, 101)

# Transformera Rs datamaterial "trees", svenska enheter och namn
träd <- trees
colnames(träd) <- c("Diameter", "Höjd", "Volym")
träd <- transform(träd, Radie = Diameter / 39.37 / 2) # Tum till meter
träd <- transform(träd, Höjd = Höjd / 3.28) # Fot till meter
träd <- transform(träd, Volym = Volym / 3.28^3) # Kubikfot till kubikmeter
träd <- subset(träd, select = -Diameter) # Ta bort diameter

```

```{r, echo = handout}
## Funktioner

# Weibull-likelihood
L.weib <- Vectorize( # Möjliggör elementvis beräkning i (alpha, beta)
  function(alpha, beta, data){
    # Bestämmer likelihood givet en vektor observationer från Weibull(alpha, beta)
    # Argument:
    #     (alpha, beta): parametervärde (eventuellt vektorer)
    #     data: observationsvektor
    # Utdata:
    #     Likelihoodfunktionens värden i (alpha, beta)
    #
    L.out <- prod(dweibull(data, shape = alpha, scale = 1 / beta))
    if (is.nan(L.out))
      L.out <- 0
    return(L.out)
  }
  , vectorize.args = c("alpha", "beta"))
l.weib <- Vectorize( # Möjliggör elementvis beräkning i theta
  function(alpha, beta, data){
    # Bestämmer loglikelihood givet en vektor observationer från Weibull(alpha, beta)
    # Argument:
    #     (alpha, beta): parametervärde (eventuellt vektorer)
    #     data: observationsvektor
    # Utdata:
    #     loglikelihoodfunktionens värden i (alpha, beta)
    #
    l.out <- sum(log(dweibull(data, shape = alpha, scale = 1 / beta)))
    return(l.out)
  }
  , vectorize.args = c("alpha", "beta"))
# Score-vektor för Weibulllikelihood
S.weib <- function(alpha, beta, data){
    # Bestämmer scorevektor givet en vektor observationer från Weibull(alpha,beta)
    # Argument:
    #     (alpha,beta): parametervärden
    #     data: observationsvektor
    # Utdata:
    #     Scorefunktionens värden i (alpha,beta)
    #
    S.out <- c(sum(1 / alpha+log(data * beta) * (1 - (data * beta)^alpha)),
               alpha * sum(1 - (data * beta)^alpha) / beta)
    return(S.out)
}
# Fisherinformation för Weibulllikelihood
I.weib <- function(alpha, beta, data){
    # Bestämmer informationsmatris givet en vektor observationer från Weibull(alpha,beta)
    # Argument:
    #     (alpha,beta): parametervärden
    #     data: observationsvektor
    # Utdata:
    #     informationsmatrisens värden i (alpha,beta)
    #
    I11 <- sum(1 / alpha^2 + log(data * beta)^2 * (data * beta)^alpha)
    I12 <- sum((data * beta)^alpha + alpha * log(data * beta) * (data * beta)^alpha - 1) / beta
    I22 <- alpha * sum((alpha - 1) * (data * beta)^alpha + 1) / beta^2
    I.out<-cbind(c(I11, I12), c(I12, I22))
    return(I.out)}
# Likelihood för modellen Volym_i=beta*Höjd_i*Radie_i^2+epsilon_i
# med epsilon_i oberoende Normal(0, sigma2)
P.träd <- Vectorize( # Möjliggör elementvis beräkning i (beta, sigma2)
  function(beta, sigma2, data){
    # Bestämmer likelihood i regressionsmodell för volymer av trädstammar
    # Argument:
    #     (beta, sigma2): parametervärde (eventuellt vektorer)
    #     data: data.frame innehållander variablerna Volym, Radie och Höjd
    # Utdata:
    #     Likelihoodfunktionens värden i (beta, sigma2)
    #    
    L.out <- with(data, prod(dnorm(Volym, beta * Radie^2 * Höjd, sd = sqrt(sigma2)))) / sigma2
    return(L.out)
  },
  vectorize.args = c("beta", "sigma2")
)
```

## Modellval, Weibull mot Exponential

- $Exponential(\beta)=Weibull(1,\beta)$, vi kan testa $\alpha=1$ i Weibullmodell.

```{r, echo = FALSE, warning=FALSE}
target <- function(theta, data){
    -l.weib(theta[1], theta[2], data)
}
theta.hat <- optim(c(1.1, 0.011), target, data = ac)$par
alpha.hat <- theta.hat[1]
beta.hat <- theta.hat[2]
beta.hat.0 <- 1 / mean(ac)
```
```{r, echo = TRUE}
T.L <- -2 * (l.weib(1, beta.hat.0, ac) - l.weib(alpha.hat, beta.hat, ac))
p <- 1 - pchisq(T.L, 1)
p
```

## Occams rakkniv (William av Occam, 1285-1347)
<div class="columns-2">
```{r, out.width = "300px"}
knitr::include_graphics("William_of_Ockham.png")
```

*Non est ponenda pluralitas sine necessitate* [krångla inte till saker i onödan]. 

Välj exponentialmodellen!
</div>


## Modellval

- hur jämför vi två modeller som inte är nästlade?
- hur jämför vi fler än två modeller med hypotestest?

## Kullback-Leibler avstånd

<div class="boxed">
**Definition:** (H & B, A.3.8) Kullback-Leibler avvikelsen av tätheten $p$ från $q$ är
$$
D(q|p)= E_q(\log(q(X)))-E_q(\log(p(X))),
$$
där $X\sim q$.
</div>

## Kullback-Leibler avstånd

Givet $m$ modeller $M_1,\ldots, M_m$ med tätheter $p_1(x), \ldots, p_m(x)$ vill vi välja modellen som minimerar
$$
D(q|p_i)=C-E_q\log(p_i(X))
$$
där $q$ är den sanna tätheten, alternativt maximerar $E_q\log(p_i(X))$.

*Problem:* Tätheterna $p_i$ beror på okända parametrar.

## Vad händer med ML-skattaren om modellen är fel?

Låt $q$ vara den sanna tätheten, ML-skattaren maximerar

$$
\frac{1}{n}\sum_{i=1}^n\log p(X_i|\theta)\approx E_q(\log p(X_i|\theta))=C-D(q|p(\cdot|\theta))
$$
d.v.s. den väljer (asymptotiskt) $\theta$ så att Kullback-Leibler avståndet till sanna tätheten minimeras.

Därför rimligt att använda $\hat{\theta}$ även när modellen är "fel".

## Exempel: Gamma anpassad till Normal

```{r, echo = TRUE}
x <- rnorm(100, mean = 4, sd = 1)
nl.gamma <- function(theta, data){
    -sum(log(dgamma(data, theta[1], theta[2])))
}
theta.hat <- optim(c(5,1), nl.gamma, data = x)$par
theta.hat
```

## Exempel: Gamma anpassad till Normal

```{r, echo = handout}
t <- seq(0, 8, length.out = 100)
hist(x, prob = TRUE, xlim = c(0,8))
lines(t, dgamma(t, shape = theta.hat[1], rate = theta.hat[2]))
```

## Exempel: Exponential anpassad till Normal

```{r, echo = handout}
t <- seq(0, 12, length.out = 100)
plot(t, dexp(t, rate = 1 / mean(x)), type = "l", ylim = c(0,.5))
hist(x, prob = TRUE, add = TRUE)
```

## Hirotugu Akaike (1927-2009)

```{r, out.width = "300px"}
knitr::include_graphics("akaike-s.jpg")
```

Akaike föreslog att maximera
$$
K(i)=E_q\log p_i(X|\hat{\theta}_i(Y))
$$
där $X$ och $Y$ är oberoende med täthet $q$.


## AIC

En naturlig skattning är 
$$
\hat{K}(i)=\frac{1}{n}\sum_{j=1}^n \log p_j(x_j|\hat{\theta}_i(x))
$$
som dock överskattar $K(i)$ då vi använder $x$ istället för $y$. Akaike visade att

$$
E_q(\hat{K}(i))-K(i)\approx \frac{p_i}{n},
$$
där $p_i$ är antalet parametrar i modell $i$ och föreslog *Akaikes informationskriterium*
$$
AIC(i)=-2n(\hat{K}(i)-\frac{p_i}{n})=-2l_i(\hat{\theta}_i)+2p_i
$$

## AIC, Exponential vs Weibull

```{r, echo = TRUE}
AIC.Weib <- -2 * l.weib(alpha.hat, beta.hat, ac) + 2 * 2
AIC.Weib
AIC.Exp <- -2 * l.weib(1, beta.hat.0, ac) + 2 * 1
AIC.Exp
```

AIC förordar Weibullmodellen.

## Dela upp i träningsdata/testdata

- Bias i $\hat{K}$ kommer sig av att vi använder samma data två ggr.
- Ett alternativ vore att dela upp data i två delar $(x,y)$ och skatta $K$ med
$$
\frac{1}{n_x}\sum_{i=1}^{n_x} \log p(x_i|\hat{\theta}_i(y)),
$$
där $\hat{\theta}_i(y)$ är ML-skattaren baserat på $y$ i modell $i$.

## Leave-one-out korsvalidering

Vanligast är
$$
K_{CV}(i)=\frac{1}{n}\sum_{j=1}^n\log p(x_j|\hat{\theta}_i(x_{-j}))
$$
där $x_{-j}$ är $x$ utan observation $j$.

## Korsvalidering, Exponential vs Weibull

```{r, echo = TRUE, warnings = FALSE}
nl.weib <- function(theta, data){-L.weib(theta[1], theta[2], data)}
K.w <- numeric(length(ac)); K.e <- numeric(length(ac))
for (j in 1:length(ac)){
    x.j <- ac[-j]
    theta.hat <- optim(c(1, 0.01), nl.weib, data = x.j)$par
    K.w[j] <- log(dweibull(ac[j], theta.hat[1], scale = 1 / theta.hat[2]))
    K.e[j] <- log(dexp(ac[j], rate = theta.hat[2]))    
}
-2 * sum(K.w) #Weibull
-2 * sum(K.e) #Exponential
```


# Bayesianskt modellval

## Modellsannolikheter

Ansätt apriorisannolikheter $P(M_i)$, aposteriori blir

$$
P(M_i|x)=\frac{p(x|M_i)p(M_i)}{\sum_{j=1}^m p(x|M_j)p(M_j)}
$$

## BIC, Bayesian Information Criterium

En approximation av den marginella likelihooden motiverar

$$
BIC(i)=-2l_i(\hat{\theta}_i)+p_i\log(n)
$$
som alternativ till AIC.

## BIC, Exponential vs Weibull

```{r, echo = TRUE}
n <- length(ac)
BIC.Weib <- -2 * l.weib(alpha.hat, beta.hat, ac) + 2 * log(n)
BIC.Weib
BIC.Exp <- -2 * l.weib(1, beta.hat.0, ac) + 1 * log(n)
BIC.Exp
```